{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcbbda9-ea15-415b-bb5f-39fcbe08d940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and connect to WRDS\n",
    "import pandas as pd\n",
    "import wrds\n",
    "\n",
    "conn = wrds.Connection()\n",
    "\n",
    "# Extract S&P 500 constituent data\n",
    "sp500 = conn.raw_sql(\"\"\"\n",
    "    select a.*, b.date, b.ret\n",
    "    from crsp.msp500list as a,\n",
    "    crsp.msf as b\n",
    "    where a.permno=b.permno\n",
    "    and b.date >= a.start and b.date<= a.ending\n",
    "    and b.date >= '01/01/2000'\n",
    "    order by date;\n",
    "\"\"\", date_cols=['start', 'ending', 'date'])\n",
    "\n",
    "# Merge company names and basic info\n",
    "mse = conn.raw_sql(\"\"\"\n",
    "    select comnam, ncusip, namedt, nameendt, \n",
    "    permno, shrcd, exchcd, hsiccd, ticker\n",
    "    from crsp.msenames\n",
    "\"\"\", date_cols=['namedt', 'nameendt'])\n",
    "mse['nameendt'] = mse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "sp500_full = pd.merge(sp500, mse, how='left', on='permno')\n",
    "sp500_full = sp500_full.loc[\n",
    "    (sp500_full.date >= sp500_full.namedt) & \n",
    "    (sp500_full.date <= sp500_full.nameendt)\n",
    "]\n",
    "\n",
    "# Link with Compustat\n",
    "ccm = conn.raw_sql(\"\"\"\n",
    "    select gvkey, liid as iid, lpermno as permno, linktype, linkprim, \n",
    "    linkdt, linkenddt\n",
    "    from crsp.ccmxpf_linktable\n",
    "    where substr(linktype,1,1)='L'\n",
    "    and (linkprim ='C' or linkprim='P')\n",
    "\"\"\", date_cols=['linkdt', 'linkenddt'])\n",
    "ccm['linkenddt'] = ccm['linkenddt'].fillna(pd.to_datetime('today'))\n",
    "\n",
    "sp500ccm = pd.merge(sp500_full, ccm, how='left', on=['permno'])\n",
    "sp500ccm = sp500ccm.loc[\n",
    "    (sp500ccm['date'] >= sp500ccm['linkdt']) &\n",
    "    (sp500ccm['date'] <= sp500ccm['linkenddt'])\n",
    "]\n",
    "\n",
    "# ----- Critical Addition: Get CIK from Compustat -----\n",
    "compustat_company = conn.raw_sql(\"\"\"\n",
    "    select gvkey, cik\n",
    "    from comp.company\n",
    "\"\"\")\n",
    "\n",
    "sp500ccm = pd.merge(\n",
    "    sp500ccm, \n",
    "    compustat_company[['gvkey', 'cik']], \n",
    "    how='left', \n",
    "    on='gvkey'\n",
    ")\n",
    "\n",
    "# Filter 2020-2024 and add quarter grouping\n",
    "sp500ccm = sp500ccm[['date', 'permno', 'comnam', 'ticker', 'gvkey', 'cik']]\n",
    "sp500ccm['date'] = pd.to_datetime(sp500ccm['date'])\n",
    "\n",
    "sp500_2020_2024 = sp500ccm[\n",
    "    (sp500ccm['date'] >= '2020-01-01') & \n",
    "    (sp500ccm['date'] <= '2024-12-31')\n",
    "].copy()\n",
    "\n",
    "sp500_2020_2024['year'] = sp500_2020_2024['date'].dt.year\n",
    "sp500_2020_2024['quarter'] = sp500_2020_2024['date'].dt.to_period('Q')\n",
    "\n",
    "# Export results\n",
    "sp500_2020_2024.to_csv('sp500_tickers_2020_2024_by_quarter.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4270f0c0-e141-45d8-bdcd-8e4da0a3da91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete! Output: merged_ticker_quarter_cik.csv\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file (assumes columns: ticker, quarter)\n",
    "csv_path = \"sp500_tickers_2020_2024_by_quarter.csv\"\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "\n",
    "# Read TXT file (tab-delimited with ticker-CIK mapping)\n",
    "txt_path = \"ticker.txt\"\n",
    "df_txt = pd.read_csv(txt_path, sep='\\t', header=None, names=['ticker', 'cik'])\n",
    "\n",
    "# Deduplicate: keep first CIK per ticker\n",
    "df_txt_unique = df_txt.drop_duplicates(subset='ticker', keep='first')\n",
    "\n",
    "# Merge data (left join to preserve all CSV records)\n",
    "merged_df = pd.merge(\n",
    "    df_csv[['ticker', 'quarter']],  # Maintain original quarter data\n",
    "    df_txt_unique,\n",
    "    on='ticker',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check for missing CIKs\n",
    "missing_cik = merged_df[merged_df['cik'].isnull()]\n",
    "if not missing_cik.empty:\n",
    "    print(\"Tickers missing CIK:\")\n",
    "    print(missing_cik['ticker'].unique())\n",
    "\n",
    "# Export results\n",
    "merged_df.to_csv(\"merged_ticker_quarter_cik.csv\", index=False)\n",
    "print(\"Merge complete! Output: merged_ticker_quarter_cik.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8849c-e05b-4056-8e7e-597d7b5447c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
